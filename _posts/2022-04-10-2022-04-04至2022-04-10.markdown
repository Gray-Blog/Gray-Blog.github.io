---
layout:     post
title:      "Weekly report from 2022.04.04 to 2022.04.10"
subtitle:   " \"Hello World, Hello Blog\""
date:       2022-4-10 21:00:00
author:     "Gray"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - Generative Model
    - Normal
    - GAN
---


## BN.LN.IN.GN

各种在N\*C\*WH上的normalization的方法，之前基本都再不同网络中见过，但是没有系统的总结过，所以去系统的了解了，看看几个normalization的方法是怎么work的。

+ Batch Normalization：这个是在卷积神经网络中比较常用的方法。目的基本都是为了调整层间输出分布，防止梯度爆炸，协变量偏移导致效果下降的问题。缺点是BN是在N维度上做归一化，如果N过小的话，每一个batch代表的样本分布可能相差过大。
+ Layer Normalization：这个方法我一般是在transformer或者RNN结构的网络中见到，是对每个样本做归一化。原因是因为RNN和transformer一般处理序列信息，每个样本序列的大小长度往往是不一样的，这和图片不同，所有样本中会有很多0填充，如果用BN的话，batch之间的波动会很大，所以一般一个一个做归一化。
+ Instance Normalization：这个方法我之前在做Gan相关的项目的时候会用到。它相当于是Layer Normalization的通道版，是对每个样本的每个通道做归一化。主要用来做风格迁移，原因是在风格迁移中，每个样本的每个像素点都很重要，所以BN和LN都不够细致，前者不用说，后者也忽略的通道信息，所以单独对每个通道做归一化。
+ Group Normalization：这个方法我之前还真没遇见过。看文献中说当batchsize小于16时比较好用。我个人看法是，该方法介于LN和IN之间，LN忽略了通道信息，而IN又过分强调通道信息，而GN则认为，通过卷积或者其它网络提取出来的特征图，在有些通道上其实提取出的特征是类似的，所以可以group在一起。

## Some papers

这周看了一下Gan和CycGan的论文，跟着b站上的白板推导课程手推了一下Gan的公式和最优解，然后用复现了一下CycGan[代码](https://github.com/gray311/Models4PyTorchLightning)（后面学了PL框架后，又用PL写了一遍）。

+ Gan的主要目的就是用一个神经网络把随机分布z拟合到我们需要的目标样本x上：$x = NN(z;\theta)$. loss函数是一个最小最大化问题。
+ CycGan稍微复杂一点，相当于两个Gan网络拼在一起，互相欺骗，最后达到一个纳什均衡。主要是在loss函数中多加入了两类loss，一个是consistency loss，目的是要求域A上的图像经过两个生成器后要尽可能靠近原来的A；一个是identity loss，目的是要求域A上的图像经过它自己域上的生成器时，要尽可能保持不变，这也是很好理解的。
+ 总结了一下我的训练经验把：一是先训练判别器D还是生成器G，先训练判别器D，有一次D forward，有两次G backward，先训练生成器，有两次D forward，一次G backward，一般来说计算一次G的梯度的复杂度往往大于计算一次判别器前向传播的复杂度高，所以我在训练的时候先训练G；二是，如果训练效果一直难以收敛，可以尝试取消identity loss，CycleGAN论文中有提到，代码也有不过默认是关闭的。个人在做训练时发现加入identity loss后往往训练很多轮epoch，当时某些图片风格变化任然不明显（比如上面的第二幅图）因此，加入identity loss在做domain adaptation这件事情上没有太好的收益，但是图像迁移的质量确实有所提升；三是，这个是我在网上看到的tips，关于resize小图的问题，在他实现从GTA->CityScapes的过程中，发现scale width或者是resize的方式，要比crop更好，因为如果resize到1024，然后再crop 400x400，也就是大家常用的配置，这种情况是不如我们scale width到600~800，然后再crop到400x400，甚至不如直接scale width到400的效果好。

总的来说Gan还是比较好懂的把，代码也挺好写的。其实感觉基于CNN的代码现在模块化的程度越来越高，基本都挺好写的。


