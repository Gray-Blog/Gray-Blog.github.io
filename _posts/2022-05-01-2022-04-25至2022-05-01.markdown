---
layout:     post
title:      "Weekly report from 2022.04.25 to 2022.05.01"
subtitle:   " \"Hello World, Hello Blog\""
date:       2022-5-01 23:00:00
author:     "Gray"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - NLP
    - BERT
---

## 代码——word2vec

这周主要是在看和复现沐神自然语言处理课程的代码（主要bert的源码是用tf写的，不太熟tf）。主要分为word2vec和bert语言模型的训练，总结了一些bert代码实现的一些细节把。

+ word2vec模型比较简单，每个样本包含4个部分，一是中心词；二是context_negatives，上下文和非上下文干扰词；三是masks，作为context_negatives的掩码，因为每个样本中的中心词的上下文和干扰词可能不一样多，所以按最多的填充，自然需要掩码，这在language model很常见；最后是label，主要是标记context_negatives中的context，而忽略negatives。
+ word2vec的训练过程，首先将centers和其对应的context_negatives编码（nn.embedding），得到u，v，两个做点积得到pred，然后做一个binary_cross_entropy_with_logits()得到loss，然后跟新梯度就行。

## 代码——bert

这一部分是bert代码的一些细节和感想.

+ 首先是准备bert预训练的数据集。根据bert原文可以知道，bert的预训练主要靠两个任务，一个是mask（完形填空，也就是bidirectional的实现方式），第二个是传统的给定上文预测下一句。在沐神给的代码中，bert的预训练集中，每个样本都包含两个segment（句子），每个句子中都有token被mask，同时也包含一个bool，表示第二个segment是否是第一个segment的真正的下一句。故而bert的数据集处理还是有点麻烦的，每个样本包含下面7个部分：

    + all_token_ids：词向量索引 512 * 64  512个样本64个词，代码每个词索引（不是词向量），放到bertencoder去编码。
    + all_segments：段索引 512 * 64 ，512个样本，64个词的段标记，标记该样本中哪些词属于第一段，哪些属于第二段，只有0/1两个值，当然建立在bert的两种预训练方式下，样本这么构造。
    + valid_lens：有效句长度 512 ，标记每个样本中前多少个词有效。 
    + all_pred_positions：512 * 10 ,表示512个样本，每个样本中最多有10个token是被mask处理过的（因为每个样本最大含64个token），值表示这10个token在句子中的位置。
    + all_mlm_weights：512 * 10 由于每个样本的长度不一样，所以其15%也不一样，因此需要对all_pred_positions填充，填充后自然需要掩码。
    + all_mlm_labels：512 * 10 表示被mask处理过的词的词索引，同样结果填充变成label，之后于pred做cross entropy就是mask任务的loss。
    + nsp_labels：512 表示每个样本中的第二个segment是否是第一个segment的真正下一句，之后与pred做binary cross entropy就是预测下一句任务的loss。

+ bert结构大致没问题，代码中有些细节。首先是self.segment_embedding = nn.Embedding(2, num_hiddens)，每个样本中的词索引和段索引都要编码，相加后得到我们熟悉的输入，然后加上位置编码。这里熟悉了我之前没怎么用过的nn.Embedding和nn.Parameter两个函数。之后就丢进几个transformer blk。这样就得到了bert的编码器。
+ 另外就是两个预训练任务的模块。第一个是MaskLM：该模块输入为bert编码器的输出encode_X 512 * 64 * 768，然后根据样本中自带的 all_pred_positions，选出之前被mask处理过的token所处位置的encode_X_pred 512 * 10 * 768。按我们的设想，该位置上的encode_X应该是原来被mask前的词。因此，我们将该encode_X_pred放入一个包含两个线形层的mlp，最后得到一个mlm_Y_hat 512 * 64 * 10000（10000为词表长度），其中数值最大的索引代表bert预测的词，之后就做cross entropy就行。
+ 第二个任务是NextSentencePred，这个很简单，直接把encode_X后两维flatten 512 * （64*768）丢进一个线性层得到ouput 512 * 2，之后和nsp_labels做一个binary cross entropy。最后两个loss加起来backward就可以开始预训练了。
  
总的来说bert模型的细节还是有不少的，但看懂其实都挺合乎常理的，nlp中相较于cv的样本，往往多包含一个mask，毕竟每个token长度不一样。这周原计划是做kaggle那个多模态的比赛，但是准备了一个很重要的面试，所以只看了bert代码。下周的计划应该是打kaggle，然后看一下对比学习或者mixer mlp这种关于transformer结构相关的论文把，到时候再说。
