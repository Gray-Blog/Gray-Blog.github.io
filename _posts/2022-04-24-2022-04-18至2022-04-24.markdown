---
layout:     post
title:      "Weekly report from 2022.04.18 to 2022.04.24"
subtitle:   " \"Hello World, Hello Blog\""
date:       2022-4-24 21:00:00
author:     "Gray"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - NLP
    - PyTorchlightning
    - Multimodal
---

## PyTorchLightning

上周了解了PyTorchLightning框架，感觉好像很不错的样子，可以基本把代码模板化，训练效率也没有下降，于是这周决定动手用这个框架写一些简单的代码，以后都按这个模板来写。

+ PL的流程很简单，生产流水线，有一个固定的顺序：初始化 def \__init\__(self) -->训练training_step(self, batch, batch_idx) --> 校验validation_step(self, batch, batch_idx) --> 测试 test_step(self, batch, batch_idx). 就完事了，总统是实现这三个函数的重写。当然，除了这三个主要的，还有一些其他的函数，为了方便我们实现其他的一些功能，因此更为完整的流程是在training_step 、validation_step、test_step 后面都紧跟着其相应的 training_step_end(self，batch_parts)和training_epoch_end(self, training_step_outputs) 函数，当然，对于校验和测试，都有相应的\*_step_end和*_epoch_end函数.上述的这些函数都集成在pl.LightningModule模块中。同时，训练数据都利用继承LightningDataModule模块的类来生成train_dataloader、val_dataloader、test_dataloader.
  
&nbsp;

+ 另外，PL框架也将很多功能便利化，比如模型保存，配置各种checkpoint；多GPU训练；半精度训练；梯度累计；batchsize缩放；梯度剪裁；小数据集训练等等，都只需要配置参数就好而不需要额外的代码。
&nbsp;
+ 简单利用PL搭建了几个语义分割的网络，这里是链接:  [代码](https://github.com/gray311/AlveolarNet)

## Kaggle Competition H&M

+ 这个比赛任务大概是利用客户的之前的购买记录，来预测客户未来的购买商品。题目给了很多数据，包括购买记录，客户信息，商品的各种信息和其配对的图片，要根据这些数据来进行预测。
看到既有图片也有文字，我首先想到这是一个多模态的工作，但具体的思路还没有清晰。所有先对这些数据做了[EDA](https://github.com/gray311/Kaggle-Competition-H-M-multimodaldal/blob/main/EDA.ipynb)，打算下周再详细构思写代码。

## Some papers

这周看了两篇论文，分别是GPT系列(GPT看了，GPT-2，GPT-3改动不大，主要是规模和加入zero-shot和few-shot的区别，所以文章没有细看，衍生的各种应用倒是很有趣)和CLIP。

+ GPT结构基于transformer的解码器，和RNN模型相比transformer在迁移学习的时候学习到的特征更加稳健一些，可能是因为其里面有更加结构化的记忆使得能够处理更长的文本信息从而能够抽取出更好的句子层面和段落层面的语义信息。无监督的预训练方式，给你一组序列预测该序列的下一个词，对比bert的完形填空的方式该任务更难，所以效果不如bert。有监督的fine-tune，两个损失函数，一个是预测下一个词，还有一个是给一个完整的句子预测标签。根据下游任务会调整输入和最后的线性层。
+ GPT-2，因为打不过bert，所以作者采用另一种创新方式，建立了一个新的数据集，引入zero-shot这个更难的任务，子任务不再需要提供相关样本fine-tune，直接采用预训练模型。
+ GPT-3，拥有175 billion参数的模型，和GPT-2一样，同样不需要fine-tune和gradient decent，不同在于对于下游任务采用few-shot。
+ CLIP，很强大的一个多模态模型，之前的网络都是通过训练去预测给定的目标类别（也就是说label是给定的），但是这种监督式的训练会限制网络的泛化能力和可用性（如果出现其他没见过的目标对象还得打上额外的标签）。所以CILP提出了将每个图片的label改为一段text描述，同时利用两个编码器从n张图片和配对的text中抽特征（image encoder可用CNN类或者transformer，text encoder用transform），然后图片特征和文字特征两两做cosine similarity，得到一个n*n的矩阵，其中矩阵对角线上的n个元素作为正样本，其他n^2-n个元素作为负样本，然后进行contrastive pre-training（为什么使用对比学习做预训练，因为更加节约训练成本，也更符合直觉。
+ CLIP模型看下来还是比较容易理解的。但它的局限性也有很多，我觉得比较重要的有三点：第一是clip的zero-shot效果远远不如现在的sota模型，所以任重道远（这个其实我觉得没啥，毕竟是泽荣-shot嘛）；第二是面对数据 out of distribution的情况，CLIP的效果不行，也就是说其实CLIP这个zero-shot本质上还是在巨大的4亿张数据集的囊括中，有点伪zero-shot的意味，比如mnist这个数据集上只有88%的准确率，说明CLIP这个模型本质上还是很脆弱的；第三是CLIP这种zero-shot还是从给定的描述中去推理图片和文字是否类似，而更好的方式是直接给定图片直接生成文字，训练一个生成模型。

总的来说CLIP给我带来的震撼不如GPT-3，因为我总感觉如果给linear probe on ResNet这个文中对比的基线网络4亿张图片以及远超1000类别的数据集来做预训练，后者的效果不一定会比CLIP差很多把。其实我就是更看好生成式模型，可惜CLIP受限于计算资源，还做不到。希望以后的工作能提供更好的解决方案吧。
